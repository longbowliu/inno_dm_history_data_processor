import os
import glob
import yaml  # éœ€è¦å®‰è£… PyYAML: pip install pyyaml
import csv
import json
import requests
from minio_upload_innopc import upload_file_to_minio ,id_generator
from minio_upload_innopc import get_db_connection, get_sl_connection


# ======================
# 1. é…ç½®éƒ¨åˆ†
# ======================
BASE_DIR = "/mnt/AIDataSet"  # æ•°æ®é›†æ ¹ç›®å½•

# ======================
# 2. å·¥å…·å‡½æ•°ï¼šå®‰å…¨è¯»å–æ–‡ä»¶
# ======================
def read_file(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        print(f"[é”™è¯¯] æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
        return None

# ======================
# 3. å„æ–‡ä»¶å¤„ç†å‡½æ•°ï¼ˆå¯é€æ­¥å®Œå–„ï¼‰
# ======================

def process_inno_pc(dataset_name, file_path, lidar_index):
    print(f"\nğŸ”§ [å¤„ç† Inno PC] æ–‡ä»¶: {file_path}")
    bucket_name = "inno-pc"
    file_name = os.path.basename(file_path)
    try:
        file_id = upload_file_to_minio(bucket_name, file_name, file_path)
        return file_id, file_name
    except Exception as e:
        print(f"[é”™è¯¯] ä¸Šä¼ æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        return None, None
    
   
def upload_innopc_generat_scene(dataset_name ,dataset_dir):
    lidar1_innopc = glob.glob(os.path.join(dataset_dir, "Lidar1",  "*.inno_pc"))
    uuid1 = None
    innopc_name1 = None
    uuid2 = None
    innopc_name2 = None
    if lidar1_innopc:
        lidar_index = 1
        uuid1,innopc_name1 = process_inno_pc(dataset_name,lidar1_innopc[0],lidar_index)
        
    lidar2_innopc = glob.glob(os.path.join(dataset_dir, "Lidar2", "*.inno_pc"))
    if lidar2_innopc:
        lidar_index = 2
        uuid2,innopc_name2 = process_inno_pc(dataset_name,lidar2_innopc[0],lidar_index)
        
    if uuid1 or uuid2:
        print(f"[ä¿¡æ¯] æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå”¯ä¸€ID: {uuid1 if uuid1 else uuid2}")
        name = dataset_name
        position = "s"
        sdk = "inno_pc_client_3.102.9_x86"
        tags = []
        innopcs = []
        if uuid1:
            innopcs.append({
                "attachId": uuid1,
                "lidarName": "Lidar_1",
                "lidarModel": None,
                "scanMode": None
            })
        if uuid2:
            innopcs.append({
                "attachId": uuid2,
                "lidarName": "Lidar_2",
                "lidarModel": None,
                "scanMode": None
            })
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŒååœºæ™¯
        connection = get_db_connection()
        if not connection:
            return None
        try:
            with connection.cursor() as cursor:
                sql = "SELECT COUNT(*) FROM dm_scene WHERE name = %s"
                cursor.execute(sql, (name,))
                result = cursor.fetchone()
                if result and result[0] > 0:
                    # å­˜åœ¨åŒååœºæ™¯ï¼Œæ·»åŠ æ—¶é—´æˆ³
                    import time
                    name = f"{name}_{int(time.time())}"
        except Exception as e:
            print(f"[é”™è¯¯] æŸ¥è¯¢åœºæ™¯åç§°å¤±è´¥: {e}")
            return None
        finally:
            if connection:
                connection.close()
        
        scene_data = {
            "name": name,
            "position": position,
            "sdk": sdk,
            "tags": tags,
            "innopcs": innopcs
        }
        scene_data_json = json.dumps(scene_data)
        print(scene_data_json)
        url = "http://localhost/dmapi/scene"
        headers = {
            "Content-Type": "application/json"
        }
        response = requests.post(url, data=scene_data_json, headers=headers)
        print(response.text)
        print(response.status_code)
        if response.status_code == 200:
            print("[ä¿¡æ¯] åœºæ™¯æ•°æ®æäº¤æˆåŠŸ")
            # æŸ¥è¯¢ dm_innopc è¡¨è·å– innopc id
            connection = get_db_connection()
            if not connection:
                return None
            try:
                with connection.cursor() as cursor:
                    innopc_ids = []
                    if uuid1:
                        sql = "SELECT id FROM dm_innopc WHERE attach_id = %s  order by create_time desc"
                        cursor.execute(sql, (uuid1,))
                        result = cursor.fetchone()
                        if result:
                            innopc_ids.append(result[0])
                    if uuid2:
                        sql = "SELECT id FROM dm_innopc WHERE attach_id = %s  order by create_time desc"
                        cursor.execute(sql, (uuid2,))
                        result = cursor.fetchone()
                        if result:
                            innopc_ids.append(result[0])
                    print(f"[ä¿¡æ¯] è·å–çš„ innopc id: {innopc_ids}")
                    return innopc_ids
            except Exception as e:
                print(f"[é”™è¯¯] æŸ¥è¯¢ innopc id å¤±è´¥: {e}")
                return None
            finally:
                if connection:
                    connection.close()
        else:
            print(f"[é”™è¯¯] åœºæ™¯æ•°æ®æäº¤å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return None
    else:
        print("[é”™è¯¯] æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
        return None

def get_scene_id():
    connection = get_db_connection()
    if not connection:
        return None
    try:    
        with connection.cursor() as cursor:
            sql = "SELECT id FROM dm_scene ORDER BY create_time DESC LIMIT 1"
            cursor.execute(sql)
            result = cursor.fetchone()
            if result:
                scene_id = result[0]
                print(f"[ä¿¡æ¯] æœ€æ–°åœºæ™¯ ID: {scene_id}")
                return scene_id
            else:
                print("[é”™è¯¯] æœªæ‰¾åˆ°åœºæ™¯è®°å½•")
                return None
    except Exception as e:
        print(f"[é”™è¯¯] æŸ¥è¯¢åœºæ™¯ ID å¤±è´¥: {e}")
        return None
    finally:
        if connection:
            connection.close()  
    
    
def upload_meta_file_get_attach_id(file_path):
    if isinstance(file_path, list):
        file_path = file_path[0]
    print(f"\nğŸ”§ [ä¸Šä¼ å…ƒæ•°æ®æ–‡ä»¶] æ–‡ä»¶: {file_path}")
    bucket_name = "meta20250616"
    file_name = os.path.basename(file_path)
    try:
        attach_id = upload_file_to_minio(bucket_name, file_name, file_path)
        return attach_id
    except Exception as e:
        print(f"[é”™è¯¯] ä¸Šä¼ æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        return None
def upload_metadata_to_scene(innopc_ids, scene_id,dataset_dir,dataset_name,group_name):
    print(f"\nğŸ”§ æ¯ä¸€ä¸ªmeta group é™¤äº†è‡ªå·±çš„é…ç½®å¤–è¿˜éœ€è¦åŠ è½½å…¶ä»–å…±ç”¨çš„åŸºç¡€é…ç½®ï¼Œæ¯”å¦‚æ‹‰å¹³ã€èåˆç­‰ã€‚å¦‚æœæ²¡æœ‰è¯¥åˆ†ç»„ï¼Œåˆ™è¯¥é…ç½®ä¼šä½œä¸ºå…±ç”¨é…ç½®")
    if not scene_id:
        print("[é”™è¯¯] æ— æ•ˆçš„åœºæ™¯ IDï¼Œæ— æ³•ä¸Šä¼ å…ƒæ•°æ®")
        return
    meta_group_data = {
        "name": group_name,
        "metaIds": [],
        "metas": [],
        "sceneId": scene_id
    }
    # 1. æŸ¥æ‰¾æ‰€æœ‰å­ç›®å½•ä¸­çš„ .zip æ–‡ä»¶
    zip_files = glob.glob(os.path.join(dataset_dir, "**", "*.zip"), recursive=True)
    if zip_files:
        print(f"[ä¿¡æ¯] æ‰¾åˆ°ä»¥ä¸‹ .zip æ–‡ä»¶: {zip_files}")
    else:
        print("[âš ï¸] æœªæ‰¾åˆ° .zip æ–‡ä»¶")

    # 2. Flatten/*.yaml
    flatten_yaml = glob.glob(os.path.join(dataset_dir, "Flatten", "*.yaml"))
    flatten_yaml_id = None
    if flatten_yaml:
        flatten_yaml_id = upload_meta_file_get_attach_id(flatten_yaml[0])
    else:
        print("[âš ï¸] æœªæ‰¾åˆ° Flatten YAML")
    if flatten_yaml_id:
        flatten_yaml_json =  {
        "attachId": flatten_yaml_id,
        "type": "flatten"
        }
        meta_group_data["metas"].append(flatten_yaml_json)
    

    # 3. Fusion/**/*.yaml
    fusion_zone_yaml = glob.glob(os.path.join(dataset_dir, "Fusion", group_name, "*.yaml"))
    fusion_zone_id = None
    if  fusion_zone_yaml:
        fusion_zone_id = upload_meta_file_get_attach_id( fusion_zone_yaml[0])
        
    if fusion_zone_id:
        fusion_zone_json =  {
        "attachId": fusion_zone_id,
        "type": "fusion_zone"
        }       
        meta_group_data["metas"].append(fusion_zone_json)
        
    # Fusion/*.yaml    
    fusion_yaml_id = None
    fusion_yaml = glob.glob(os.path.join(dataset_dir, "Fusion", "*.yaml"))
    if fusion_yaml:
        fusion_yaml_id = upload_meta_file_get_attach_id(fusion_yaml[0])
    else:
        print("[âš ï¸] æœªæ‰¾åˆ° Fusion YAML")
    if fusion_yaml_id:
        fusion_yaml_json =  {
        "attachId": fusion_yaml_id,
        "type": "fusion"
        }       
        meta_group_data["metas"].append(fusion_yaml_json)

 

    # 5. Lidar1/**/*.yaml
    lidar1_roi = glob.glob(os.path.join(dataset_dir, "Lidar1", group_name, "*.yaml"))
    if not lidar1_roi:
        lidar1_roi = glob.glob(os.path.join(dataset_dir, "Lidar1", "*.yaml"))
    lidar1_roi_id = None
    if lidar1_roi:    
        idar1_roi_id = upload_meta_file_get_attach_id(lidar1_roi[0])
    else:
        print("[âš ï¸] æœªæ‰¾åˆ° Lidar1 ROI YAML æ–‡ä»¶")
    if idar1_roi_id:
        lidar1_roi_json =  {
        "attachId": idar1_roi_id,
        "type": "lidar_zone",       
        "innopcId": innopc_ids[0] if innopc_ids else None
        }
        meta_group_data["metas"].append(lidar1_roi_json)
    

    # 6. Lidar2/**/*.yaml
    lidar2_roi = glob.glob(os.path.join(dataset_dir, "Lidar2", group_name, "*.yaml"))
    if not lidar2_roi:
        lidar2_roi = glob.glob(os.path.join(dataset_dir, "Lidar2", "*.yaml"))
    lidar2_roi_id = None
    if lidar2_roi:
        lidar2_roi_id = upload_meta_file_get_attach_id(lidar2_roi[0])
    else:
        print("[âš ï¸] æœªæ‰¾åˆ° Lidar2 ROI YAML æ–‡ä»¶")
    if lidar2_roi_id:
        lidar2_roi_json =  {
        "attachId": lidar2_roi_id,
        "type": "lidar_zone",       
        "innopcId": innopc_ids[1] if innopc_ids and len(innopc_ids) >1 else None
        }
        meta_group_data["metas"].append(lidar2_roi_json)

    # 7. ParamServer/**/params_*.yaml
    param_dir = glob.glob(os.path.join(dataset_dir, "ParamServer", group_name,"*.yaml"))
    if not param_dir:
        param_dir = glob.glob(os.path.join(dataset_dir, "ParamServer", "*.yaml"))
    param_id = None
    if param_dir:       
        param_id = upload_meta_file_get_attach_id(param_dir[0])
    else:
        print("[âš ï¸] æœªæ‰¾åˆ° ParamServer YAML æ–‡ä»¶")
    if param_id:
        param_json =  {
        "attachId": param_id,
        "type": "params"
        }       
        meta_group_data["metas"].append(param_json)
    # 8. static_map/static_*.pcd
    static_map_pcd = glob.glob(os.path.join(dataset_dir, "static_map",group_name, "*.pcd"))
    if not static_map_pcd:
        static_map_pcd = glob.glob(os.path.join(dataset_dir, "static_map", "*.pcd"))
    static_map_pcd_id = None
    if static_map_pcd:
        static_map_pcd_id = upload_meta_file_get_attach_id(static_map_pcd[0])
    else:
        print("[âš ï¸] æœªæ‰¾åˆ° static_map PCD æ–‡ä»¶")
    if static_map_pcd_id:
        static_map_pcd_json =  {
        "attachId": static_map_pcd_id,
        "type": "static_map_pcd"
        }
        meta_group_data["metas"].append(static_map_pcd_json)
        
    # 9. label_zone (å¯é€‰ï¼Œè§†å…·ä½“éœ€æ±‚æ·»åŠ )      
    label_zone_file = glob.glob(os.path.join(dataset_dir, "BoxFilterROI", group_name, "*.yaml"))
    if not label_zone_file:
        label_zone_file = glob.glob(os.path.join(dataset_dir, "BoxFilterROI", "*.yaml"))
    label_zone_id = None    
    if label_zone_file:   
        label_zone_id = upload_meta_file_get_attach_id(label_zone_file)
        if label_zone_id:
            label_zone_json =  {
            "attachId": label_zone_id,
            "type": "label_zone"
            }
            meta_group_data["metas"].append(label_zone_json)
    else:
        print("[âš ï¸] æœªæ‰¾åˆ° LabelZone YAML æ–‡ä»¶")
        
    #10. other, scene_config.yaml
    scene_config_id = None
    scene_config = glob.glob(os.path.join(dataset_dir, "**", "scene_config.yaml"), recursive=True)
    if scene_config:
        scene_config_id = upload_meta_file_get_attach_id(scene_config[0])
    else:
        print("[âš ï¸] æœªæ‰¾åˆ° Fusion YAML")
    if scene_config_id:
        scene_config_json =  {
        "attachId": scene_config_id,
        "type": "other"
        }       
        meta_group_data["metas"].append(scene_config_json)
        
    metadata_json = json.dumps(meta_group_data)
    print(f"[ä¿¡æ¯] å‡†å¤‡ä¸Šä¼ å…ƒæ•°æ®åˆ°åœºæ™¯ ID {scene_id}")
    print(f"[ä¿¡æ¯] å…ƒæ•°æ® JSON: {metadata_json}")
    url = f"http://localhost/dmapi/group"
    headers = {
        "Content-Type": "application/json"
    }
    response = requests.post(url, data=metadata_json, headers=headers)
    print(response.text)
    print(response.status_code)
    if response.status_code == 200:
        print("[ä¿¡æ¯] å…ƒæ•°æ®ä¸Šä¼ æˆåŠŸ")
    else:
        print(f"[é”™è¯¯] å…ƒæ•°æ®ä¸Šä¼ å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
        
        
# ======================
# 4. ä¸»ç¨‹åºï¼šéå†æ•°æ®é›†å¹¶åˆ†å‘å¤„ç†
# ======================
def process_dataset(dataset_name ,dataset_dir):
    print("=" * 50)
    print(f"ğŸ” æ­£åœ¨å¤„ç†æ•°æ®é›†ç›®å½•: {dataset_dir}")
    print("=" * 50)
    innopc_ids = upload_innopc_generat_scene(dataset_name ,dataset_dir)
    scene_id = get_scene_id()
    
    lidar1_path = os.path.join(dataset_dir, "Lidar1")
    # éå† Lidar1 ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å¤¹ï¼Œæ¯ä¸ªæ–‡ä»¶å¤¹ä½œä¸ºä¸€ä¸ª group_name !!!
    meta_group_names = [d for d in os.listdir(lidar1_path) if os.path.isdir(os.path.join(lidar1_path, d))]
    group_id_default = None
    group_id = None
    # GT é»˜è®¤å…³è”default meta_group, æ ¹æ®åˆ†æ,æ¯ä¸ªæ•°æ®é›†ä¸­åªå­˜åœ¨ä¸€ä¸ªGT zip æ–‡ä»¶(ä¸€èˆ¬å­˜åœ¨äº dataset_dir/æˆ–è€… dataset_dir/Fusion/ ä¸‹) 
    # å¦‚æœä¸å­˜åœ¨default groupï¼Œåˆ™æ„å‘³ç€åªæœ‰ä¸€ç»„meta_groupï¼Œå…³è”GTæ•°æ®åˆ°è¯¥group_id
    group_records = [] 
    for group_name in meta_group_names:
        print(f"[ä¿¡æ¯] å¤„ç†åˆ†ç»„: {group_name}")
        upload_metadata_to_scene(innopc_ids,scene_id,dataset_dir,dataset_name ,group_name)
        group_id = find_group_id_by_name_and_scene_id(scene_id, group_name)
        group_records.append((group_name, group_id))
        if group_name == "default" :
            group_id_default = group_id
    if group_id_default:
        group_id = group_id_default
    requirement_id = create_requirements( dataset_name, group_id, scene_id)
    
    zip_files = glob.glob(os.path.join(dataset_dir, "**", "*.zip"), recursive=True)
    if zip_files:
        print(f"[ä¿¡æ¯] æ‰¾åˆ°ä»¥ä¸‹ .zip æ–‡ä»¶: {zip_files}")
        gt_file = zip_files[0]
        upload_gt(gt_file,requirement_id)
    else:
        print("[âš ï¸] æœªæ‰¾åˆ° .zip GTæ–‡ä»¶")
    manual = False
    for group_name, group_id in group_records:
        event_csv_files = glob.glob(os.path.join(dataset_dir, "event_gt", group_name, "*.csv"))
        for event_csv_file in event_csv_files:
            event_type = None
            if event_csv_file.endswith("advance_detection.csv"):
                event_type = "advance_detection"
            elif event_csv_file.endswith("stop_bar.csv"):
                event_type = "stop_bar"
            else:
                print(f"[âš ï¸] æœªçŸ¥çš„äº‹ä»¶æ–‡ä»¶ç±»å‹: {event_csv_file}")
                continue    
            print(f"[ä¿¡æ¯] å¤„ç†äº‹ä»¶æ–‡ä»¶: {event_csv_file} for group: {group_name}")
              
            upload_event_by_group(requirement_id,event_csv_file,event_type,group_id,manual)
            
def upload_event_by_group(equirement_id,event_csv_file,event_type,group_id,manual=False):
    
    print(f"\nğŸ”§ [ä¸Šä¼  Event æ–‡ä»¶] æ–‡ä»¶: {event_csv_file}, éœ€æ±‚ ID: {equirement_id}, äº‹ä»¶ç±»å‹: {event_type}")
    file_name = os.path.basename(event_csv_file)
    
    try:    
        # æ„é€  multipart/form-data è¯·æ±‚
        url = "http://localhost/dmapi/cognition-truth/upload"
        data = {
            "requirementId": equirement_id,
            "eventType": event_type,
            "groupId": group_id,
            "manual": manual
        }
        
        with open(event_csv_file, 'rb') as f:
            files = {'file': (file_name, f, 'text/csv')}
            response = requests.post(url, data=data, files=files)
            print(response.text)
            print(f"[ä¿¡æ¯] çŠ¶æ€ç : {response.status_code}")
            if response.status_code == 200:
                print("[ä¿¡æ¯] Event æ–‡ä»¶æäº¤æˆåŠŸ")
                return True
            else:
                print(f"[é”™è¯¯] Event æ–‡ä»¶æäº¤å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False
    except Exception as e:
        print(f"[é”™è¯¯] ä¸Šä¼  Event æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False    
    
    

        
    return

def upload_gt(gt_file, requirement_id):
    """
    ä¸Šä¼  GT æ–‡ä»¶åˆ°åç«¯æ¥å£
    param file: GT zipæ–‡ä»¶è·¯å¾„
    param requirement_id: éœ€æ±‚ ID
    return: ä¸Šä¼ ç»“æœï¼ˆæˆåŠŸæˆ–å¤±è´¥ï¼‰
    """
    print(f"\nğŸ”§ [ä¸Šä¼  GT æ–‡ä»¶] æ–‡ä»¶: {gt_file}, éœ€æ±‚ ID: {requirement_id}")
    # bucket_name = "gt-files"
    file_name = os.path.basename(gt_file)
    # è§£å‹gt_file,éå†å…¶ä¸­.txtæ–‡ä»¶çš„æ•°é‡ä½œä¸ºframe_count,æ±‡æ€»æ¯ä¸ª.txtæ–‡ä»¶ä¸­çš„è¡Œæ•°ï¼ˆæ¯è¡Œä¸ºä¸€ä¸ªboxï¼‰ä½œä¸ºbox_countã€‚
    # å¦‚æœgt_fileä¸­æ²¡æœ‰.txtæ–‡ä»¶ï¼Œåˆ™box_countå’Œframe_countä¸º-1 ï¼Œæœ€ååˆ é™¤è§£å‹çš„æ–‡ä»¶å¤¹
    box_count = 0
    frame_count = 0
    import zipfile
    import shutil
    temp_extract_dir = os.path.join(os.path.dirname(gt_file), "temp_gt_extract")
    try:
        with zipfile.ZipFile(gt_file, 'r') as zip_ref:
            zip_ref.extractall(temp_extract_dir)
        txt_files = glob.glob(os.path.join(temp_extract_dir, "**", "*.txt"), recursive=True)
        frame_count = len(txt_files)
        for txt_file in txt_files:
            with open(txt_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                box_count += len(lines)
        print(f"[ä¿¡æ¯] è®¡ç®—å¾—åˆ° frame_count: {frame_count}, box_count: {box_count}")
    except Exception as e:
        print(f"[é”™è¯¯] è®¡ç®— GT æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}")
    finally:
        if os.path.isdir(temp_extract_dir):
            shutil.rmtree(temp_extract_dir)       
    
    
    try:

        # æ„é€  multipart/form-data è¯·æ±‚
        url = "http://localhost/dmapi/perception-truth/upload"
        data = {
            "requirementId": requirement_id,
            "boxCount": box_count,
            "frameCount": frame_count
        }
        
        with open(gt_file, 'rb') as f:
            files = {'file': (file_name, f, 'application/zip')}
            response = requests.post(url, data=data, files=files)
            print(response.text)
            print(f"[ä¿¡æ¯] çŠ¶æ€ç : {response.status_code}")
            if response.status_code == 200:
                print("[ä¿¡æ¯] GT æ–‡ä»¶æäº¤æˆåŠŸ")
                return True
            else:
                print(f"[é”™è¯¯] GT æ–‡ä»¶æäº¤å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False

        
    except Exception as e:
        print(f"[é”™è¯¯] GT æ–‡ä»¶ä¸Šä¼ æˆ–æäº¤å¤±è´¥: {e}")
        return False
        



def create_requirements(dataset_name, group_id, scene_id):
    '''
    æ’å…¥æ•°æ®åˆ° ad_sl_requirement å’Œ dm_requirement_scene_group è¡¨ä¸­
    '''
    if not group_id or not scene_id:
        print("[é”™è¯¯] æ— æ•ˆçš„ group ID æˆ– scene IDï¼Œæ— æ³•åˆ›å»ºéœ€æ±‚æ–‡ä»¶")
        return
    requirement_id = None
    requirement_scene_group_id = None
    # è·å–æ•°æ®åº“è¿æ¥
    connection = get_sl_connection()
    if not connection:
        print("[é”™è¯¯] æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
        return

    try:
        with connection.cursor() as cursor:
            # æ’å…¥æ•°æ®åˆ° ad_sl_requirement è¡¨
            requirement_sql = """
                INSERT INTO ad_sl_requirement (name, config_id, priority, simpl_version, lost_info, create_time, update_time,is_deleted)
                VALUES (%s, %s, %s, %s, %s, NOW(), NOW(),1)
            """
            cursor.execute(requirement_sql, (
                f"{dataset_name}_demand",  # name
                "1792801536928169986",     # config_id
                3,                          # priority
                "SIMPL_2_6",               # simpl_version
                None                        # lost_info
            ))
            requirement_id = cursor.lastrowid
            print(f"[ä¿¡æ¯] æ’å…¥ ad_sl_requirement è¡¨æˆåŠŸï¼ŒID: {requirement_id}")
        # æäº¤äº‹åŠ¡
        connection.commit()
        print("[ä¿¡æ¯] éœ€æ±‚æ–‡ä»¶åˆ›å»ºæˆåŠŸ")

    except Exception as e:
        print(f"[é”™è¯¯] æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
        if connection:
            connection.rollback()
    finally:
        if connection:
            connection.close()   
            
     # è·å–æ•°æ®åº“è¿æ¥
    connection = get_db_connection()
    if not connection:
        print("[é”™è¯¯] æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
        return

    try:
        with connection.cursor() as cursor:
            # æ’å…¥æ•°æ®åˆ° ad_sl_requirement è¡¨

            # æ’å…¥æ•°æ®åˆ° dm_requirement_scene_group è¡¨
            requirement_scene_group_sql = """
                INSERT INTO dm_requirement_scene_group (id, requirement_id, scene_id, group_id, create_time, update_time)
                VALUES (%s, %s, %s, %s, NOW(), NOW())
            """
            cursor.execute(requirement_scene_group_sql, (
                id_generator.generate_id(),  # id
                requirement_id,    # requirement_id
                scene_id,          # scene_id
                group_id           # group_id
            ))
            print("[ä¿¡æ¯] æ’å…¥ dm_requirement_scene_group è¡¨æˆåŠŸ")

        # æäº¤äº‹åŠ¡
        connection.commit()
        print("[ä¿¡æ¯] éœ€æ±‚æ–‡ä»¶åˆ›å»ºæˆåŠŸ")
        return requirement_id

    except Exception as e:
        print(f"[é”™è¯¯] æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
        if connection:
            connection.rollback()
    finally:
        if connection:
            connection.close()   
        
        
def find_group_id_by_name_and_scene_id(scene_id, group_name):
    connection = get_db_connection()
    if not connection:
        return None
    try:
        with connection.cursor() as cursor:
            sql = "SELECT id FROM dm_group WHERE scene_id = %s AND name = %s ORDER BY create_time DESC LIMIT 1"
            cursor.execute(sql, (scene_id, group_name))
            result = cursor.fetchone()
            if result:
                group_id = result[0]
                print(f"[ä¿¡æ¯] å…ƒæ•°æ®åˆ†ç»„ ID: {group_id}")
                return group_id
            else:
                print("[é”™è¯¯] æœªæ‰¾åˆ°å…ƒæ•°æ®åˆ†ç»„è®°å½•")
                return None
    except Exception as e:
        print(f"[é”™è¯¯] æŸ¥è¯¢å…ƒæ•°æ®åˆ†ç»„ ID å¤±è´¥: {e}")
        return None
    finally:
        if connection:
            connection.close()
            

def check_inno_pc_files(dataset_dir,dest_file="innopc_empty.txt"):
    """
    æ£€æŸ¥ Lidar1 å’Œ Lidar2 ç›®å½•ä¸‹æ˜¯å¦æœ‰ .inno_pc æ–‡ä»¶
    å¦‚æœæ²¡æœ‰ï¼Œåˆ™å°†æ•°æ®é›†æ ¹ç›®å½•å†™å…¥ innopc_empty.txt æ–‡ä»¶
    """
    lidar1_inno_pc = glob.glob(os.path.join(dataset_dir, "Lidar1", "*.inno_pc"))
    lidar2_inno_pc = glob.glob(os.path.join(dataset_dir, "Lidar2", "*.inno_pc"))
    if not lidar1_inno_pc or not lidar2_inno_pc:
        with open(dest_file, "a") as f:
            f.write(os.path.basename(dataset_dir) + "\n")



def mount_nas():
        """
        æŒ‚è½½ NAS å­˜å‚¨åˆ°æœ¬åœ°ç›®å½• /mnt
        """
        nas_path = "//172.16.98.52/inno_test_storage"
        mount_point = "/mnt"
        username = "share"
        password = "a12345678"
        command = f"sudo mount -t cifs {nas_path} {mount_point} -o username={username},password={password}"
        try:
            os.system(command)
            print(f"[ä¿¡æ¯] NAS æŒ‚è½½æˆåŠŸ: {nas_path} -> {mount_point}")
        except Exception as e:
            print(f"[é”™è¯¯] NAS æŒ‚è½½å¤±è´¥: {e}")
            return False
        return True
#'Lidar1', 'static_map', 'ParamServer', 'Lidar2', 'InnoPCClient', 'Lidar3', 'BoxFilterROI', 'Fusion', 'scene_config.yaml', 'Lidar4', 'Flatten'
folders_types = ['Lidar1', 'static_map', 'ParamServer', 'Lidar2', 'Lidar3', 'BoxFilterROI', 'Fusion', 'Lidar4', 'Flatten','event_gt']

def meta_group_analysis(dataset_path):
    print("\n"+f"[ä¿¡æ¯] meta group åˆ†æ: {dataset_path}")  # for if condition test
    # print("\n"+f"[ä¿¡æ¯] æœªå¤„ç†æ–‡ä»¶: {dataset_path}")   # for else condition test
    for type_folder_name in os.listdir(dataset_path):
        if type_folder_name  in folders_types:
            type_folder = os.path.join(dataset_path, type_folder_name)
            if os.path.isdir(type_folder):
                sub = []
                for meta_group_name in os.listdir(type_folder):
                    if(os.path.isdir(os.path.join(type_folder,meta_group_name))):
                        # print(meta_group_name)
                        sub.append(meta_group_name)
                print("  ->["+type_folder_name+"], size="+str(len(sub))+" , detail: "+str(sub))
        # else:
        #      print(f"   -> {type_folder_name} ")
    
# ======================
# 5. ä¸»å…¥å£
# ======================
def main():
   
    # 1. æŒ‚è½½ NAS
    # if not mount_nas():
    #     return
    
    # BASE_DIR = "/mnt/AIDataSet"
    BASE_DIR = "/home/demo/data/test"  # For testing
    if not os.path.isdir(BASE_DIR):
        print(f"[é”™è¯¯] æ•°æ®é›†æ ¹ç›®å½•ä¸å­˜åœ¨: {BASE_DIR}")
        return
    innopc_empty_folder = os.path.join(os.getcwd(), "analysis/innopc_empty.txt")
    # é¢„åŠ è½½ innopc_empty.txt æ–‡ä»¶å†…å®¹åˆ°é›†åˆ
    with open(innopc_empty_folder, "r") as f:
        empty_innopc_set = set(line.strip() for line in f)
    
    
    processed_folder = os.path.join(os.getcwd(), "analysis/processed_datasets.txt")
    # é¢„åŠ è½½ innopc_empty.txt æ–‡ä»¶å†…å®¹åˆ°é›†åˆ
    with open(processed_folder, "r") as f:
        processed_set = set(line.strip() for line in f)
        
    # éå† BASE_DIR ä¸‹çš„æ¯ä¸ªæ•°æ®é›†æ–‡ä»¶å¤¹ï¼ˆå¦‚ A01_001_2_FK_Sï¼‰
    for item in os.listdir(BASE_DIR):
        dataset_path = os.path.join(BASE_DIR, item)
        #  !!!åªè¿è¡Œä¸€æ¬¡ æ£€æŸ¥ Lidar1 å’Œ Lidar2 ç›®å½•ä¸‹æ˜¯å¦æœ‰ .inno_pc æ–‡ä»¶, ç”Ÿæˆ analysis/innopc_empty.txt
        # check_inno_pc_files(dataset_path,innopc_empty_folder)
        not_empty_innopc = os.path.basename(dataset_path) not in empty_innopc_set
        not_processed = os.path.basename(dataset_path) not in processed_set
        if os.path.isdir(dataset_path) and not_empty_innopc and not_processed:
            #  !!!åªè¿è¡Œä¸€æ¬¡ meta group åˆ†æï¼Œ ç”Ÿæˆ analysis/meta_group_analysis.txt å’Œ analysis/not_handle_yet.txt
            # meta_group_analysis(dataset_path)   
            process_dataset(item,dataset_path)
            # æŠŠitemè¿½åŠ åˆ° analysis/processed_datasets.txt æ–‡ä»¶ä¸­ï¼Œè¡¨ç¤ºå·²ç»å¤„ç†è¿‡è¯¥æ•°æ®é›†
            processed_datasets_file = os.path.join(os.getcwd(), "analysis/processed_datasets.txt")
            with open(processed_datasets_file, "a") as f:
                f.write(item + "\n")
            print(f"[ä¿¡æ¯] æ•°æ®é›†å¤„ç†å®Œæˆ: {dataset_path}") 
        else:
            print(f"[è·³è¿‡] ç©ºç›®å½•æˆ–è€…ç¼ºå¤± innopc æ–‡ä»¶: {dataset_path}")
        

if __name__ == "__main__":
    main()